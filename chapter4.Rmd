# Week 4. Clustering and classification

### Task 2

```{r warning=FALSE, message=FALSE}
# Set working directory
setwd("Z:/Jatko-opinnot/IODS/IODS-project-1")
#setwd("C:/Users/sirja/Documents/IODS-project-1")

# Access needed libraries
library("MASS")
library("tidyverse")
library("corrplot")

```

```{r}
# load the data
data("Boston")

# explore the dataset
str(Boston)
```

Dataset "Boston" contains housing values in suburbs of Boston. It is a matrix with 506 rows and 14 columns. Rows are towns in Boston and columns are variables such as crime rate  or pupil-teacher ratio by town.
More information on the data can be found [here](https://stat.ethz.ch/R-manual/R-devel/library/MASS/html/Boston.html).


### Task 3
```{r}
# calculate the correlation matrix and round it
cor_matrix<-cor(Boston) 

# print the correlation matrix
cor_matrix %>% round(digits = 2)

# visualize the correlation matrix
corrplot(cor_matrix, method="circle", type = "upper", cl.pos = "b", tl.pos = "d", tl.cex = 0.6)

```

This correlation plot shows the correlation between the variables. 

>  "[The corrplot package](https://cran.r-project.org/web/packages/corrplot/vignettes/corrplot-intro.html) is a graphical display of a correlation matrix, confidence interval. It also contains some algorithms to do matrix reordering. In addition, corrplot is good at details, including choosing color, text labels, color labels, layout, etc. 
Positive correlations are displayed in blue and negative correlations in red color. Color intensity and the size of the circle are proportional to the correlation coefficients."

Strongest *negative* correlation can be seen between variables dis & age, dis & nox, dis & indus.
Variables with strongest *positive* correlation are rad & tax, nox & indus, nox & age, tax & indus, rm & medv.


### Task 4
Standardize the dataset.
```{r}
# center and standardize variables
boston_scaled <- scale(Boston)

# summaries of the scaled variables
summary(boston_scaled)

# check the class of the boston_scaled object
class(boston_scaled)

# change the object to data frame
boston_scaled <- as.data.frame(boston_scaled)
# summary of the scaled crime rate
summary(boston_scaled$crim)

```

Variables change so that their mean is always zero, and maximum and minimum values are more in a similar scale, approximately from -5 to 10.


Create a categorical variable of the crime rate with quantiles as the break points.
```{r}
# create a quantile vector of crim and print it
bins <- quantile(boston_scaled$crim)
bins

# create a categorical variable 'crime'
crime <- cut(boston_scaled$crim, breaks = bins, include.lowest = TRUE, label = c("low", "med_low", "med_high", "high"))

# look at the table of the new factor crime
crime

# remove original crim from the dataset
boston_scaled <- dplyr::select(boston_scaled, -crim)

# add the new categorical value to scaled data
boston_scaled <- data.frame(boston_scaled, crime)

```


Divide the dataset to train and test sets, so that 80% of the data belongs to the train set.
```{r}
# number of rows in the Boston dataset 
n <- nrow(boston_scaled)

# choose randomly 80% of the rows
ind <- sample(n,  size = n * 0.8)

# create train set
train <- boston_scaled[ind,]

# create test set 
test <- boston_scaled[-ind,]

# save the correct classes from test data
correct_classes <- test$crime

# remove the crime variable from test data
test <- dplyr::select(test, -crime)
```

### Task 5
Fit the linear discriminant analysis on the train set with categorical crime rate as the target variable.
```{r}
# linear discriminant analysis
lda.fit <- lda(crime ~ ., data = train)

# print the lda.fit object
lda.fit

# the function for lda biplot arrows
lda.arrows <- function(x, myscale = 1, arrow_heads = 0.1, color = "red", tex = 0.75, choices = c(1,2)){
  heads <- coef(x)
  arrows(x0 = 0, y0 = 0, 
         x1 = myscale * heads[,choices[1]], 
         y1 = myscale * heads[,choices[2]], col=color, length = arrow_heads)
  text(myscale * heads[,choices], labels = row.names(heads), 
       cex = tex, col=color, pos=3)
}

# target classes as numeric
classes <- as.numeric(train$crime)

# plot the lda results
plot(lda.fit, dimen = 2, col = classes, pch = classes)
lda.arrows(lda.fit, myscale = 2)

```


### Task 6
```{r}
# predict classes with test data
lda.pred <- predict(lda.fit, newdata = test)

# cross tabulate the results
table(correct = correct_classes, predicted = lda.pred$class)
```

The first time I did the prediction, the classifier predicted classes *low* and *high* correctly. For *med_high* and *med_low* there were several false predictions. I tried the prediction again few times and there were false predictions for all other classes but high. This model seems to predict high crime rates accurately, but with lower crime rates there is more uncertainty.


### Task 7

```{r}
data(Boston)

# standardize the dataset
boston_scaled <- scale(Boston)
boston_scaled <- as.data.frame(boston_scaled)
```

Calculate Euclidean distances between the observations
```{r}
# euclidean distance matrix
dist_eu <- dist(boston_scaled)

# look at the summary of the distances
summary(dist_eu)
```


Run k-means algorithm
```{r}
# k-means clustering
km <-kmeans(boston_scaled, centers = 3)

```

Investigate what is the optimal number of clusters and run the algorithm again.

```{r}
set.seed(123)

# determine the number of clusters
k_max <- 10

# calculate the total within sum of squares
twcss <- sapply(1:k_max, function(k){kmeans(boston_scaled, k)$tot.withinss})

# visualize the results
qplot(x = 1:k_max, y = twcss, geom = 'line')
```

Based on the curve two is a good number of clusters.

```{r}
# k-means clustering
km <-kmeans(boston_scaled, centers = 2)

```

Visualize the clusters
```{r}
# plot the Boston dataset with clusters
pairs(boston_scaled, col = c("#fc90d9", "#8de6a0")[km$cluster], lower.panel = NULL) 

```

Let's call the clustered towns pink and green towns and see how the different variables vary between these towns. 
Crime rates are low in the pink towns, in green towns there is more variation in crime rates. In the green towns, there is more industry and also more nitrogen oxides in the air than in the pink towns.
In pink towns there are larger apartments and the proportion of blacks is low, whereas in green towns the proportion of black inhabitants vary more. In green towns the median value of owner-occupied homes is lower than in pink towns.
Based on these results it seems that there is inequality between the towns of Boston.

***